{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Introducing Tensorflow\n",
    "\n",
    "Be sure to \"pip install tensorflow\" or \"pip install tensorflow-gpu\" first!\n",
    "\n",
    "## The world's simplest Tensorflow application\n",
    "\n",
    "Let's begin by writing a really simple program to illustrate Tensorflow's main concepts. We'll set up two Variables, named \"a\" and \"b\", which each contain a tensor which contains a single value - the number 1, and the number 2.\n",
    "\n",
    "We then create a graph \"f\" that adds these two tensors together. But \"f = a + b\" just creates the graph; it doesn't actually perform the addition yet.\n",
    "\n",
    "Next we need to initialize any global variables before we run the graph.\n",
    "\n",
    "And finally, we create a Tensorflow Session object, run our variable initializer, and execute the graph with eval(). \n",
    "\n",
    "This returns the sum of 1 + 2 in a rather complex, yet highly scalable manner :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "a = tf.Variable(1, name=\"a\")\n",
    "b = tf.Variable(2, name=\"b\")\n",
    "f = a + b\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as s:\n",
    "    init.run()\n",
    "    print( f.eval() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## And now for something more interesting: Handwriting recognition\n",
    "\n",
    "The standard example for machine learning these days is the MNIST data set, a collection of 70,000 handwriting samples of the numbers 0-9. Our challenge - to predict which number each handwritten image represents.\n",
    "\n",
    "Although we'll talk about neural networks that are specifically well suited for image recognition later, we actually don't need to go there for this relatively simple task. We can achieve decent without a whole lot of code.\n",
    "\n",
    "Each image is 28x28 grayscale pixels, so we can treat each image as just a 1D array, or tensor, of 784 numbers. As long as we're consistent in how we flatten each image into an array, it'll still work. Yes, it would be even better if we could preserve the 2D structure of the data while training - but we'll get there later.\n",
    "\n",
    "Let's start by importing the data set, which conveniently is part of tensorflow itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "MNIST provides 55,000 samples in a training data set, 10,000 samples in a test data set, and 5,000 samples in a \"validation\" data set. We haven't talked about validation sets before, but their intent is to be used for model selection. So you'd use validation data to select your model, train the model with the training set, and then evaluate the model using the test data set.\n",
    "\n",
    "If you're new to the concept of train/test - it's important to evaluate the performance of our neural network using data it's never seen before. Otherwise it's kinda like giving students a math test for problems they already have the answers for. So, we use a completely different set of images to train our neural network from the images used for testing its accuracy.\n",
    "\n",
    "The training data is therefore a tensor of shape [55,000, 784] - 55,000 instances of 784 numbers that represent each image.\n",
    "\n",
    "The test data is encoded as \"one_hot\" when we loaded it above. Think of one_hot as a binary representation of the label data - that is, which number each handwriting sample was intended to represent. Mathematically one_hot represents a dimension for every possible label value. Every dimension is set to the value 0, except for the \"correct\" one which is set to 1. For example, the label vector representing the number 1 would be [0, 1, 0, 0, 0, 0, 0, 0, 0, 0] (remember we start counting at 0.) It's just a format that's optimized for how the labels are applied during training.\n",
    "\n",
    "So the test data is a tensor of shape [55,000, 10] - 55,000 test images each associated with 10 binary values that indicate whether or not the image represents a given number from 0-9.\n",
    "\n",
    "Let's define a little function to let us visualize what the input data looks like, and pick some random training image to see what it is we're up against:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_sample(num):\n",
    "    #Print the one-hot array of this sample's label \n",
    "    print(mnist.train.labels[num])  \n",
    "    #Print the label converted back to a number\n",
    "    label = mnist.train.labels[num].argmax(axis=0)\n",
    "    #Reshape the 768 values to a 28x28 image\n",
    "    image = mnist.train.images[num].reshape([28,28])\n",
    "    plt.title('Sample: %d  Label: %d' % (num, label))\n",
    "    plt.imshow(image, cmap=plt.get_cmap('gray_r'))\n",
    "    plt.show()\n",
    "    \n",
    "display_sample(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "So, you can see the \"one_hot\" representation of the number this image represents in the array that was printed out from the training label for image# 1234, as well as what this particular sample looks like. You can tell that some of the training data would even be challenging for a human to classify!\n",
    "\n",
    "Go ahead and try different input images to get a feel of the data we're given. Any value between 0 and 55,000 will work.\n",
    "\n",
    "As a reminder, we're flattening each image to a 1D array of 784 (28 x 28) numerical values. Each one of those values will be an input node into our deep neural network. Let's visualize how the data is being fed into it just to drive that point home:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "images = mnist.train.images[0].reshape([1,784])\n",
    "for i in range(1, 500):\n",
    "    images = np.concatenate((images, mnist.train.images[i].reshape([1,784])))\n",
    "plt.imshow(images, cmap=plt.get_cmap('gray_r'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This is showing the first 500 training samples, one on each row. Imagine each pixel on each row getting fed into the bottom layer of a neural network 768 neurons (or \"units\") wide as we train our neural network.\n",
    "\n",
    "So let's start setting up that artificial neural network. We'll start by creating \"placeholders\" for the input images and for the \"correct\" labels for each. Think of these as parameters - we build up our neural network model without knowledge of the actual data that will be fed into it; we just need to construct it in such a way that our data will fit in.\n",
    "\n",
    "So our \"input_images\" placeholder will be set up to hold an array of values that consist of 784 floats (28x28), and our \"target_labels\" placeholder will be set up to hold an array of values that consist of 10 floats (one-hot format for 10 digits.)\n",
    "\n",
    "While training, we'll assign input_images to the training images and target_labels to the training lables. While testing, we'll use the test images and test labels instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "input_images = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "target_labels = tf.placeholder(tf.float32, shape=[None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "So let's set up our deep neural network. We'll need an input layer with one node per input pixel per image, or 784 nodes. That will feed into a hidden layer of some arbitrary size - let's pick 512. That hidden layer will output 10 values, corresonding to scores for each classification to be fed into softmax.\n",
    "\n",
    "We'll need to reserve variables to keep track of the all the weights and biases for both layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "hidden_nodes = 512\n",
    "\n",
    "input_weights = tf.Variable(tf.truncated_normal([784, hidden_nodes]))\n",
    "input_biases = tf.Variable(tf.zeros([hidden_nodes]))\n",
    "\n",
    "hidden_weights = tf.Variable(tf.truncated_normal([hidden_nodes, 10]))\n",
    "hidden_biases = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now let's set up the neural network itself. We'll define the input layer and associate it with our placeholder for input data. All this layer does is multiply these inputs by our input_weight tensor which will be learned over time.\n",
    "\n",
    "Then we'll feed that into our hidden layer, which applies the ReLU activation function to the weighted inputs with our learned biases added in as well.\n",
    "\n",
    "Finally our output layer, called digit_weights, multiplies in the learned weights of the hidden layer and adds in the hidden layer's bias term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "input_layer = tf.matmul(input_images, input_weights)\n",
    "hidden_layer = tf.nn.relu(input_layer + input_biases)\n",
    "digit_weights = tf.matmul(hidden_layer, hidden_weights) + hidden_biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Make sure you noodle on the above block. This sets up a deep neural network like the one we talked about in our slides.\n",
    "\n",
    "output layer\n",
    "\n",
    "hidden layer\n",
    "\n",
    "input layer\n",
    "\n",
    "Next we will define our loss function for use in measuring our progress in gradient descent: cross-entropy, which applies a logarithmic scale to penalize incorrect classifications much more than ones that are close. Remember digit_weights is the output of our final layer, and we're comparing that against the target labels used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "loss_function = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=digit_weights, labels=target_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we will set up our gradient descent optimizer, initializing it with an aggressive learning rate (0.5) and our loss function defined above.\n",
    "\n",
    "That learning rate is an example of a hyperparameter that may be worth experimenting with and tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Next we'll want to train our neural network and measure its accuracy. First let's define some methods for measuring the accuracy of our trained model. \n",
    "\n",
    "correct_prediction will look at the output of our neural network (in digit_weights) and choose the label with the highest value, and see if that agrees with the target label given. During testing, digit_weights will be our prediction based on the test data we give the network, and target_labels is a placeholder that we will assign to our test labels. Ultimately this gives us a 1 for every correct classification, and a 0 for every incorrect classification.\n",
    "\n",
    "\"accuracy\" then takes the average of all the classifications to produce an overall score for our model's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(digit_weights,1), tf.argmax(target_labels,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's train this thing and see how it works! \n",
    "\n",
    "We'll set up a Tensorflow session, and initialize our variables. Next we will train our network in 2000 steps (or \"epochs\") with batches of 100 samples from our training data. At each step, we assign the input_images placeholder to the current batch of training images, and the target_labels placeholder to the current batch of training labels.\n",
    "\n",
    "Once training is complete, we'll measure the accuracy of our model using the accuracy graph we defined above. While measuring accuracy, we assign the input_images placeholder to our test images, and the target_labels placeholder to our test labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.global_variables_initializer().run()\n",
    "\n",
    "for x in range(2000):\n",
    "    batch = mnist.train.next_batch(100)\n",
    "    optimizer.run(feed_dict={input_images: batch[0], target_labels: batch[1]})\n",
    "    if ((x+1) % 100 == 0):\n",
    "        print(\"Training epoch \" + str(x+1))\n",
    "        print(\"Accuracy: \" + str(accuracy.eval(feed_dict={input_images: mnist.test.images, target_labels: mnist.test.labels})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "You should have about 92% accuracy. Not bad! But hey, we're just starting.\n",
    "\n",
    "Let's take a look at some of the misclassified images and see just how good or bad our model is, compared to what your own brain can do. We'll go through the first 100 test images and look at the ones that are misclassified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for x in range(100):\n",
    "    # Load a single test image and its label\n",
    "    x_train = mnist.test.images[x,:].reshape(1,784)\n",
    "    y_train = mnist.test.labels[x,:]\n",
    "    # Convert the one-hot label to an integer\n",
    "    label = y_train.argmax()\n",
    "    # Get the classification from our neural network's digit_weights final layer, and convert it to an integer\n",
    "    prediction = sess.run(digit_weights, feed_dict={input_images: x_train}).argmax()\n",
    "    # If the prediction does not match the correct label, display it\n",
    "    if (prediction != label) :\n",
    "        plt.title('Prediction: %d Label: %d' % (prediction, label))\n",
    "        plt.imshow(x_train.reshape([28,28]), cmap=plt.get_cmap('gray_r'))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To be honest, I'd be a little unsure about some of those myself!\n",
    "\n",
    "## Exercise\n",
    "\n",
    "See if you can improve upon the accuracy. Try using more hidden neurons (nodes). Try using fewer! Try a different learning rate. Try adding another hidden layer. Try different batch sizes. What's the best accuracy you can get from this multi-layer perceptron?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
